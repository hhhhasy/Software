import os
import time
import tempfile
from typing import Optional, Dict, Any, List
from fastapi import FastAPI, HTTPException, File, UploadFile
from pydantic import BaseModel
import whisper
import pyttsx3
import cv2
import numpy as np
import joblib
import mediapipe as mp

class HeadShakeDetector:
    def __init__(self, shake_threshold=15, buffer_len=10):
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.shake_threshold = shake_threshold
        self.buffer_len = buffer_len
        self.yaw_buffer = []

    def get_head_yaw(self, landmarks, image_shape):
        """
        è®¡ç®—å¤´éƒ¨çš„åèˆªè§’åº¦
        """
        # ä½¿ç”¨å·¦å³å¤ªé˜³ç©´ç‚¹ä½ç½®ä¼°è®¡åèˆªè§’
        left = landmarks[234]
        right = landmarks[454]
        dx = left.x - right.x
        dy = left.y - right.y
        angle = np.degrees(np.arctan2(dx, dy))
        return angle
    
    def detect(self, image):
        """
        æ£€æµ‹å›¾åƒä¸­æ˜¯å¦æœ‰æ‘‡å¤´è¡Œä¸º
        """
        h, w = image.shape[:2]
        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb)
        
        if not results.multi_face_landmarks:
            return False, image

        lm = results.multi_face_landmarks[0].landmark
        yaw = self.get_head_yaw(lm, (h, w))
        
        # ç»´æŠ¤è§’åº¦ç¼“å†²åŒº
        self.yaw_buffer.append(yaw)
        if len(self.yaw_buffer) > self.buffer_len:
            self.yaw_buffer.pop(0)

        # æ£€æµ‹è§’åº¦å˜åŒ–æ˜¯å¦è¶…è¿‡é˜ˆå€¼
        min_yaw, max_yaw = min(self.yaw_buffer), max(self.yaw_buffer)
        shake_detected = (max_yaw - min_yaw) > self.shake_threshold

        # åœ¨å›¾åƒä¸Šæ·»åŠ è°ƒè¯•ä¿¡æ¯
        cv2.putText(image, f"Yaw: {yaw:.1f}", (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)
        status = "Shake" if shake_detected else "Still"
        cv2.putText(image, f"Status: {status}", (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)
        
        return shake_detected, image
        
class UserLogin(BaseModel):
    username: str
    password: str

class UserRegister(UserLogin):
    confirm_password: str

# ============= åº”ç”¨åˆå§‹åŒ– =============

app = FastAPI(
    title="æ™ºèƒ½é©¾é©¶åŠ©æ‰‹ API",
    description="æä¾›è¯­éŸ³è¯†åˆ«ã€å¤´éƒ¨å§¿æ€ç›‘æµ‹å’Œæ‰‹åŠ¿è¯†åˆ«åŠŸèƒ½",
    version="1.0.0"
)

# åŠ è½½Whisperæ¨¡å‹
model = whisper.load_model("turbo")

# åˆå§‹åŒ–è¯­éŸ³å¼•æ“
engine = pyttsx3.init()

# ç¡¬ç¼–ç ç”¨æˆ·æ•°æ®
users = [
    {"username": "admin", "password": "admin123", "role": "admin"},
    {"username": "user1", "password": "user123", "role": "user"}
]

# ============= APIè·¯ç”±å®šä¹‰ =============

@app.post("/api/login")
async def login(user: UserLogin):
    for u in users:
        if u["username"] == user.username and u["password"] == user.password:
            return {"role": u["role"]}
    raise HTTPException(status_code=401, detail="ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")

@app.post("/api/register")
async def register(user: UserRegister):
    if user.password != user.confirm_password:
        raise HTTPException(status_code=400, detail="å¯†ç ä¸ä¸€è‡´")
    
    if any(u["username"] == user.username for u in users):
        raise HTTPException(status_code=400, detail="ç”¨æˆ·åå·²å­˜åœ¨")
    
    users.append({
        "username": user.username,
        "password": user.password,
        "role": "user"
    })
    return {"message": "æ³¨å†ŒæˆåŠŸ"}

@app.post("/api/speech-to-text")
async def speech_to_text(audio: UploadFile = File(...)) -> Dict[str, str]:
    """
    è¯­éŸ³è½¬æ–‡æœ¬APIï¼Œè¯†åˆ«è¯­éŸ³æŒ‡ä»¤å¹¶æ‰§è¡Œç›¸åº”æ“ä½œ
    
    æ¥å—éŸ³é¢‘æ–‡ä»¶ï¼Œè¿”å›è¯†åˆ«çš„æ–‡æœ¬æˆ–æŒ‡ä»¤å“åº”
    """
    try:
        # ä¿å­˜ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp:
            content = await audio.read()
            tmp.write(content)
            tmp_path = tmp.name
        
        # è¯­éŸ³è¯†åˆ«é…ç½®
        default_options = {
            "initial_prompt": "è¿™æ˜¯æ™®é€šè¯è¯­éŸ³è¯†åˆ«"
        }
        
        # ä½¿ç”¨Whisperè¿›è¡Œè¯­éŸ³è¯†åˆ«
        result = model.transcribe(tmp_path, language='zh', **default_options)
        os.unlink(tmp_path)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        
        # å®šä¹‰æŒ‡ä»¤æ˜ å°„å­—å…¸
        command_mapping = {
            "æ‰“å¼€ç©ºè°ƒ": "å·²ç»æ‰“å¼€ç©ºè°ƒ",
            "å…³é—­ç©ºè°ƒ": "å·²ç»å…³é—­ç©ºè°ƒ",
            "è°ƒé«˜æ¸©åº¦": "å·²ç»è°ƒé«˜æ¸©åº¦",
            "è°ƒä½æ¸©åº¦": "å·²ç»è°ƒä½æ¸©åº¦"
        }
        
        # æ£€æŸ¥è¯†åˆ«ç»“æœæ˜¯å¦åŒ¹é…é¢„è®¾æŒ‡ä»¤
        recognized_text = result["text"]
        for command, response in command_mapping.items():
            if command in recognized_text:
                print(f"åŒ¹é…åˆ°æŒ‡ä»¤: {command}")
                engine.say(response)  # ä½¿ç”¨è¯­éŸ³å¼•æ“è¿›è¡Œè¯­éŸ³è¾“å‡º
                engine.runAndWait()
                return {"text": response}
        
        # æœªåŒ¹é…åˆ°æŒ‡ä»¤ï¼Œè¿”å›åŸå§‹è¯†åˆ«æ–‡æœ¬
        return {"text": recognized_text}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è¯­éŸ³è¯†åˆ«é”™è¯¯: {str(e)}")

@app.post('/api/process-video')
async def process_video():
    """
    å¤´éƒ¨å§¿æ€ç›‘æµ‹APIï¼Œæ£€æµ‹é©¾é©¶å‘˜æ˜¯å¦æœ‰æ‘‡å¤´è¡Œä¸º
    
    æ‰“å¼€æ‘„åƒå¤´ï¼Œå®æ—¶ç›‘æµ‹é©¾é©¶å‘˜å¤´éƒ¨å§¿æ€
    """
    try:
        # åˆ›å»ºæ‘‡å¤´æ£€æµ‹å™¨
        detector = HeadShakeDetector(shake_threshold=15, buffer_len=15)

        # åˆå§‹åŒ–æ‘„åƒå¤´
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            raise HTTPException(status_code=500, detail="æ— æ³•è®¿é—®æ‘„åƒå¤´")
            
        shake_detected = False

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            shake, annotated_frame = detector.detect(frame)
            cv2.imshow('Head Shake Detection', annotated_frame)

            if shake:
                print("ğŸš¨ æ£€æµ‹åˆ°æ‘‡å¤´è¡Œä¸ºï¼")
                shake_detected = True
                break

            if cv2.waitKey(1) & 0xFF == 27:  # ESCé”®é€€å‡º
                break

        cap.release()
        cv2.destroyAllWindows()

        if shake_detected:
            print("âš ï¸ æç¤ºï¼šè¯·å‹¿åœ¨é©¾é©¶æ—¶æ‘‡å¤´æ™ƒè„‘ï¼")
            engine.say("è¯·å‹¿åœ¨é©¾é©¶æ—¶æ‘‡å¤´æ™ƒè„‘ï¼è¯·é›†ä¸­æ³¨æ„åŠ›ã€‚")
            engine.runAndWait()
            return {'warning': 'è¯·é›†ä¸­æ³¨æ„åŠ›ï¼'}
        else:
            return {'status': 'æ­£å¸¸'}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è§†é¢‘å¤„ç†é”™è¯¯: {str(e)}")

@app.post('/api/process-gesture')
async def process_gesture():
    """
    æ‰‹åŠ¿è¯†åˆ«APIï¼Œè¯†åˆ«å¹¶å“åº”ç”¨æˆ·çš„æ‰‹åŠ¿æ§åˆ¶
    
    æ‰“å¼€æ‘„åƒå¤´ï¼Œè¯†åˆ«é¢„å®šä¹‰çš„æ‰‹åŠ¿
    """
    try:
        GESTURES = ['fist', 'palm', 'thumbs_up', 'OK']
        
        # åŠ è½½æ‰‹åŠ¿è¯†åˆ«æ¨¡å‹
        model_path = os.path.join(os.path.dirname(__file__), 'model', 'gesture_model.pkl')
        
        try:
            clf = joblib.load(model_path)
        except FileNotFoundError:
            raise HTTPException(status_code=500, detail="æ‰‹åŠ¿è¯†åˆ«æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")

        # åˆå§‹åŒ–MediaPipeæ‰‹éƒ¨æ£€æµ‹
        mp_hands = mp.solutions.hands
        hands = mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=1,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.7
        )
        mp_draw = mp.solutions.drawing_utils

        # æ‰“å¼€æ‘„åƒå¤´
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            raise HTTPException(status_code=500, detail="æ— æ³•è®¿é—®æ‘„åƒå¤´")

        recognized_label = None
        display_start = None

        while True:
            ret, img = cap.read()
            if not ret:
                break

            h, w = img.shape[:2]  # è·å–å›¾åƒå°ºå¯¸ï¼Œé¿å…MediaPipeè­¦å‘Š
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            results = hands.process(img_rgb)

            # å¦‚æœæ£€æµ‹åˆ°æ‰‹åŠ¿å¹¶ä¸”è¿˜æœªè¯†åˆ«è¿‡
            if results.multi_hand_landmarks and recognized_label is None:
                lm = results.multi_hand_landmarks[0]
                row = []
                for p in lm.landmark:
                    row += [p.x, p.y, p.z]
                pred = clf.predict([row])[0]
                recognized_label = GESTURES[pred]
                display_start = time.time()  # è®°å½•å¼€å§‹æ˜¾ç¤ºæ–‡å­—çš„æ—¶é—´
                mp_draw.draw_landmarks(img, lm, mp_hands.HAND_CONNECTIONS)

            # å¦‚æœå·²ç»è¯†åˆ«åˆ°æ‰‹åŠ¿ï¼Œåˆ™æŒç»­åœ¨çª—å£ä¸­æ˜¾ç¤º 1 ç§’
            if recognized_label is not None:
                cv2.putText(
                    img,
                    recognized_label,
                    (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    2,
                    (0, 255, 0),
                    3
                )
                # æ˜¾ç¤ºè¶…è¿‡ 1 ç§’åé€€å‡º
                if time.time() - display_start > 1.0:
                    break

            cv2.imshow('Gesture Recognition', img)
            # æŒ‰ Esc ä¹Ÿå¯ä»¥æå‰é€€å‡º
            if cv2.waitKey(1) & 0xFF == 27:
                break

        # é‡Šæ”¾èµ„æº
        cap.release()
        cv2.destroyAllWindows()
        return {'gesture': recognized_label}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ‰‹åŠ¿è¯†åˆ«é”™è¯¯: {str(e)}")

# ============= åº”ç”¨å¯åŠ¨ =============

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)