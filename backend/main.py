from fastapi import FastAPI, HTTPException, File, UploadFile
from pydantic import BaseModel
from typing import Optional
import whisper
import tempfile
import os
import pyttsx3
import mediapipe as mp
import cv2
import numpy as np

class HeadShakeDetector:
    def __init__(self, shake_threshold=15, buffer_len=10):
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.shake_threshold = shake_threshold  # degrees of yaw change to count as shake
        self.buffer_len = buffer_len
        self.yaw_buffer = []

    def get_head_yaw(self, landmarks, image_shape):
        # Use left (234) and right (454) temple points for yaw approximation
        left = landmarks[234]
        right = landmarks[454]
        # Compute vector difference
        dx = left.x - right.x
        dy = left.y - right.y
        # Yaw angle approximation via arctan2
        angle = np.degrees(np.arctan2(dx, dy))
        return angle

    def detect(self, image):
        h, w = image.shape[:2]
        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb)
        if not results.multi_face_landmarks:
            return False, image

        lm = results.multi_face_landmarks[0].landmark
        yaw = self.get_head_yaw(lm, image.shape)
        # maintain buffer
        self.yaw_buffer.append(yaw)
        if len(self.yaw_buffer) > self.buffer_len:
            self.yaw_buffer.pop(0)

        # Check change in yaw over buffer
        min_yaw, max_yaw = min(self.yaw_buffer), max(self.yaw_buffer)
        shake_detected = (max_yaw - min_yaw) > self.shake_threshold

        # Draw indicators
        cv2.putText(image, f"Yaw: {yaw:.1f}", (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)
        status = "Shake" if shake_detected else "Still"
        cv2.putText(image, f"Status: {status}", (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)
        return shake_detected, image
        

app = FastAPI()

# åŠ è½½Whisperæ¨¡å‹
model = whisper.load_model("turbo")

# åˆå§‹åŒ–è¯­éŸ³å¼•æ“
engine = pyttsx3.init()

# ç¡¬ç¼–ç ç”¨æˆ·æ•°æ®
users = [
    {"username": "admin", "password": "admin123", "role": "admin"},
    {"username": "user1", "password": "user123", "role": "user"}
]

class UserLogin(BaseModel):
    username: str
    password: str

class UserRegister(UserLogin):
    confirm_password: str

@app.post("/api/login")
async def login(user: UserLogin):
    for u in users:
        if u["username"] == user.username and u["password"] == user.password:
            return {"role": u["role"]}
    raise HTTPException(status_code=401, detail="ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")

@app.post("/api/register")
async def register(user: UserRegister):
    if user.password != user.confirm_password:
        raise HTTPException(status_code=400, detail="å¯†ç ä¸ä¸€è‡´")
    
    if any(u["username"] == user.username for u in users):
        raise HTTPException(status_code=400, detail="ç”¨æˆ·åå·²å­˜åœ¨")
    
    users.append({
        "username": user.username,
        "password": user.password,
        "role": "user"
    })
    return {"message": "æ³¨å†ŒæˆåŠŸ"}

@app.post("/api/speech-to-text")
async def speech_to_text(audio: UploadFile = File(...)) -> dict:
    try:
        # ä¿å­˜ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp:
            tmp.write(await audio.read())
            tmp_path = tmp.name
        

        default_options = {
            "initial_prompt": "è¿™æ˜¯æ™®é€šè¯è¯­éŸ³è¯†åˆ«"
        }
            
        
        # ä½¿ç”¨Whisperè¿›è¡Œè¯­éŸ³è¯†åˆ«
        result = model.transcribe(tmp_path, language='zh',**default_options)
        os.unlink(tmp_path)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        
        # å®šä¹‰æŒ‡ä»¤æ˜ å°„å­—å…¸
        command_mapping = {
            "æ‰“å¼€ç©ºè°ƒ": "å·²ç»æ‰“å¼€ç©ºè°ƒ",
            "å…³é—­ç©ºè°ƒ": "å·²ç»å…³é—­ç©ºè°ƒ",
            "è°ƒé«˜æ¸©åº¦": "å·²ç»è°ƒé«˜æ¸©åº¦",
            "è°ƒä½æ¸©åº¦": "å·²ç»è°ƒä½æ¸©åº¦"
        }
        
        # æ£€æŸ¥è¯†åˆ«ç»“æœæ˜¯å¦åŒ¹é…é¢„è®¾æŒ‡ä»¤
        recognized_text = result["text"]
        for command, response in command_mapping.items():
            if command in recognized_text:
                print(f"åŒ¹é…åˆ°æŒ‡ä»¤: {command}")
                engine.say("%s" % response)  # ä½¿ç”¨è¯­éŸ³å¼•æ“è¿›è¡Œè¯­éŸ³è¾“å‡º
                engine.runAndWait()
                return {"text": response}
        
        # æœªåŒ¹é…åˆ°æŒ‡ä»¤ï¼Œè¿”å›åŸå§‹è¯†åˆ«æ–‡æœ¬
        return {"text": recognized_text}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è¯­éŸ³è¯†åˆ«é”™è¯¯: {str(e)}")

@app.post('/api/process-video')
async def process_video():
    # åˆ›å»ºæ‘‡å¤´æ£€æµ‹å™¨
    detector = HeadShakeDetector(shake_threshold=15, buffer_len=15)

    # åˆå§‹åŒ–æ‘„åƒå¤´
    cap = cv2.VideoCapture(0)
    shake_detected = False

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        shake, annotated_frame = detector.detect(frame)
        cv2.imshow('Head Shake Detection', annotated_frame)

        if shake:
            print("ğŸš¨ æ£€æµ‹åˆ°æ‘‡å¤´è¡Œä¸ºï¼")
            shake_detected = True
            break

        if cv2.waitKey(1) & 0xFF == 27:
            break

    cap.release()
    cv2.destroyAllWindows()

    if shake_detected:
        print("âš ï¸ æç¤ºï¼šè¯·å‹¿åœ¨é©¾é©¶æ—¶æ‘‡å¤´æ™ƒè„‘ï¼")
        engine.say("è¯·å‹¿åœ¨é©¾é©¶æ—¶æ‘‡å¤´æ™ƒè„‘ï¼è¯·é›†ä¸­æ³¨æ„åŠ›ã€‚")
        engine.runAndWait()
        return {'warning': 'è¯·é›†ä¸­æ³¨æ„åŠ›ï¼'}
    else:
        return {'status': 'æ­£å¸¸'}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)